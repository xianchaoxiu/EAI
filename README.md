# Large Language Models for Navigation (LLM4NAV)



I currently focus on large language models for navigation  including
- [Surveys](#Surveys)
- [Perception](#Perception)
- [Planning](#Planning)
- [Control](#Control)
- [Deployment](#Deployment)
- [Tools](#Tools)

<strong> Last Update: 2025/03/02 </strong>



<a name="Surveys" />

## Surveys
- [2025] The Role of World Models in Shaping Autonomous Driving: A Comprehensive Survey, arXiv [[Paper](https://arxiv.org/abs/2502.10498)] [[Code](https://github.com/LMD0311/Awesome-World-Model)]
- [2025] A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models, arXiv [[Paper](https://arxiv.org/abs/2502.17516)]
- [2025] Embodied Intelligence: A Synergy of Morphology, Action, Perception and Learning, ACM Computing Surveys  [[Paper](https://dl.acm.org/doi/abs/10.1145/3717059)] 
- [2025] Large Language Models for Multi-Robot Systems: A Survey, arXiv [[Paper](https://arxiv.org/abs/2502.03814)] [[Code](https://github.com/Zhourobotics/LLM-MRS-survey)]
- [2025] Survey on Large Language Model Enhanced Reinforcement Learning: Conceptaxonomy, and Methods, IEEE TNNLS  [[Paper](https://ieeexplore.ieee.org/abstract/document/10766898/)] 
- [2025] Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning, arXiv [[Paper](https://arxiv.org/pdf/2501.02116)] 
- [2025] UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility, arXiv [[Paper](https://arxiv.org/abs/2501.02341)] [[Code](https://github.com/Hub-Tian/UAVs_Meet_LLMs)]
- [2025] A Survey of World Models for Autonomous Driving, arXiv [[Paper](https://arxiv.org/abs/2501.11260)] 
- [2025] Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities, arXiv [[Paper](https://arxiv.org/abs/2501.09686)] 
- [2025] A Survey on Large Language Models with some Insights on their Capabilities and Limitations, arXiv [[Paper](https://arxiv.org/abs/2501.04040)]
- [2025] 基于大模型的具身智能系统综述, 自动化学报 [[Paper](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240542)]
- [2025] 具身智能研究的关键问题: 自主感知、行动与进化, 自动化学报 [[Paper](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240364)] [[Code](https://github.com/BUCT-IUSRC/Survey__EmbodiedAI)]
- [2024] 大模型驱动的具身智能: 发展与挑战, 中国科学 [[Paper](https://doi.org/10.1360/SSI-2024-0076)]
- [2024] Efficient Large Language Models: A Survey, TMLR [[Paper](https://arxiv.org/abs/2312.03863)] [[Code](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)]
- [2024] Personalization of Large Language Models: A Survey, arXiv [[Paper](https://arxiv.org/pdf/2411.00027)] 
- [2024] A Survey on LLM Inference-Time Self-Improvement, arXiv [[Paper](https://arxiv.org/abs/2412.14352)] 
- [2024] Embodied Navigation with Multi-modal Information: A Survey from Tasks to Methodology, Information Fusion [[Paper](https://www.sciencedirect.com/science/article/pii/S1566253524003105)] 
- [2024] Recent Advances in Robot Navigation via Large Language Models: A Review, arXiv [[Paper](https://www.researchgate.net/profile/Xian-Wei-3/publication/384537380)] 
- [2024] Large Language Models for Robotics: Opportunities, Challenges, and Perspectives, arXiv [[Paper](https://arxiv.org/abs/2401.04334)]
- [2024] Advances in Embodied Navigation Using Large Language Models: A Survey, arXiv [[Paper](https://arxiv.org/pdf/2311.00530)]  
- [2024] Foundation Models in Robotics: Applications, Challenges, and the Future, IJRR [[Paper](https://doi.org/10.1177/02783649241281508)] [[Code](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models)]
- [2024] A Survey of Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2303.18223)] [[Code](https://github.com/RUCAIBox/LLMSurvey)]
- [2024] ChatGPT for Robotics: Design Principles and Model Abilities, IEEE Access [[Paper](https://ieeexplore.ieee.org/abstract/document/10500490)] 
- [2023] Large Language Models for Robotics: A Survey, arXiv [[Paper](https://arxiv.org/abs/2311.07226)]




<a name="Perception" />

## Perception
- [2025] ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration, arXiv [[Paper](https://arxiv.org/abs/2502.19250)] [[Code](https://objectvla.github.io/)]
- [2025] LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token, arXiv [[Paper](https://arxiv.org/abs/2501.03895)] [[Code](https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b)]
- [2025] Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling, arXiv [[Paper](https://arxiv.org/abs/2501.17811)] [[Code](https://github.com/deepseek-ai/Janus)]
- [2024] OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding, arXiv [[Paper](https://arxiv.org/abs/2404.11000)] 
- [2024] NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models, AAAI [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28597)] 
- [2024] OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments, arXiv [[Paper](https://arxiv.org/abs/2403.09412)]  
- [2023] Chat with the Environment: Interactive Multimodal Perception using Large Language Models, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10342363)]
- [2023] VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models, arXiv [[Paper](https://arxiv.org/abs/2307.05973)]  
- [2023] Steve-Eye: Equipping LLM-Based Embodied Agents with Visual Perception in Open Worlds, ICLR [[Paper](https://arxiv.org/pdf/2310.13255)] 
- [2023] LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action, CORL [[Paper](https://doi.org/10.1177/02783649241281508)] 
- [2022] Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)] 
- [2021] Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation, arXiv [[Paper](https://arxiv.org/pdf/2104.13921)]  



<a name="Planning" />

## Planning
- [2025] MapNav: A Novel Memory Representation via Annotated Semantic Maps for VLM-based Vision-and-Language Navigation, arXiv [[Paper](https://arxiv.org/abs/2502.13451)] 
- [2025] NavRAG: Generating User Demand Instructions for Embodied Navigation through Retrieval-Augmented LLM, arXiv [[Paper](https://arxiv.org/abs/2502.11142)] 
- [2025] LlamaV-o1: Rethinking Step-by-step Visual Reasoning in LLMs, arXiv [[Paper](https://arxiv.org/abs/2501.06186)]  [[Video](https://github.com/mbzuai-oryx/LlamaV-o1)]
- [2025] Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives, arXiv [[Paper](https://arxiv.org/abs/2501.04003)]  [[Video](https://drive-bench.github.io/)]
- [2025] SD++: Enhancing Standard Defnition Mapsby Incorporating Road Knowledge using LLMs, arXiv [[Paper](https://arxiv.org/abs/2502.02773)]  
- [2025] FAST: Efficient Action Tokenization for Vision-Language-Action Models, arXiv [[Paper](https://arxiv.org/abs/2501.09747)]   [[Video](https://www.pi.website/research/fast)]
- [2025] AdaWM: Adaptive World Model based Planning for Autonomous Driving, arXiv [[Paper](https://arxiv.org/abs/2501.13072)] 
- [2025] Generative Planning with 3D-vision Language Pre-training for End-to-End Autonomous Driving, AAAI [[Paper](https://arxiv.org/abs/2501.08861)] 
- [2025] LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models, arXiv [[Paper](https://arxiv.org/pdf/2501.15850)]   [[Video](https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view)]
- [2024] Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs, EMNLP [[Paper](https://arxiv.org/abs/2406.14282)] 
- [2024] Mastering Board Games by External and Internal Planning with Language Models, arXiv [[Paper](https://arxiv.org/abs/2412.12119)] 
- [2024] TopV-Nav: Unlocking the TopView Spatial Reasoning Potential of MLLM for Zero-shot Obiect Navigation, arXiv [[Paper](https://arxiv.org/abs/2411.16425)] 
- [2024] The One RING: a Robotic Indoor Navigation Generalist, arXiv [[Paper](https://arxiv.org/pdf/2412.14401)]  [[Video](https://one-ring-policy.allen.ai/)]
- [2024] Asynchronous Large Language Model Enhanced Planner for Autonomous Driving, ECCV [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-72764-1_2)]
- [2024] Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving, arXiv [[Paper](https://arxiv.org/pdf/2412.18511)]
- [2024] LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning, arXiv [[Paper](https://arxiv.org/abs/2407.02511)]  
- [2024] SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments, ICAPS [[Paper](https://ojs.aaai.org/index.php/ICAPS/article/view/31506)] 
- [2024] AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10611163)] 
- [2023] ProgPrompt: Generating Situated Robot Task Plans using Large Language Models, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10161317)] 
- [2023] Text2Motion: from Natural Language Instructions to Feasible Plans, Autonomous Robots [[Paper](https://link.springer.com/article/10.1007/s10514-023-10131-7)] 
- [2023] LLM as A Robotic Brain: Unifying Egocentric Memory and Control, arXiv [[Paper](https://arxiv.org/abs/2304.09349)]  
- [2023] PaLM-E: An Embodied Multimodal Language Model, arXiv [[Paper](https://arxiv.org/abs/2303.03378)]  
- [2022] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, arXiv [[Paper](https://arxiv.org/abs/2204.01691)]  
- [2022] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents, ICML [[Paper](https://proceedings.mlr.press/v162/huang22a.html)]  
- [2021] Learning a Decision Module by Imitating Driver’s Control Behaviors, CORL [[Paper](https://proceedings.mlr.press/v155/huang21a.html)]  
- [2021] Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design, CORL [[Paper](https://proceedings.mlr.press/v155/sun21a.html)]  
- [2021] A Lifelong Learning Approach to Mobile Robot Navigation, IEEE RAL [[Paper](https://ieeexplore.ieee.org/abstract/document/9345478)]  



<a name="Control" />

## Control
- [2024] π0: A Vision-Language-Action Flow Model for General Robot Control, arXiv [[Paper](https://arxiv.org/abs/2410.24164)]  [[Video](https://www.physicalintelligence.company/blog/pi0)] 
- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation, arXiv [[Paper](https://arxiv.org/abs/2412.04453)]  [[Video](https://navila-bot.github.io/)] 
- [2024] Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation, arXiv [[Paper](https://arxiv.org/abs/2403.08282)]  
- [2024] GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment, arXiv [[Paper](https://arxiv.org/abs/2403.11075)]  
- [2024] Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction, arXiv [[Paper](https://arxiv.org/abs/2402.15368)]  
- [2024] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration, arXiv [[Paper](https://arxiv.org/abs/2406.14097)]
- [2024] Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610676)]  
- [2024] LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination, AAMAS [[Paper](https://arxiv.org/abs/2312.15224)]  
- [2024] VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View, AAAI [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/29858)]  
- [2024] SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning, arXiv [[Paper](https://arxiv.org/abs/2403.15648)]
- [2024] RoCo: Dialectic Multi-Robot Collaboration with Large Language Models, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610855)]  
- [2024] Building Cooperative Embodied Agents Modularly with Large Language Models, ICLR [[Paper](https://arxiv.org/abs/2307.02485)]  
- [2024] Lifelong Robot Learning with Human Assisted Language Planners, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610225)]  
- [2024] MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning, arXiv [[Paper](https://arxiv.org/abs/2402.11260)]
- [2024] LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10802075)]
- [2023] Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model, arXiv [[Paper](https://arxiv.org/abs/2305.11176)]  
- [2023] NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10341395)]
- [2023] Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration, arXiv [[Paper](https://arxiv.org/abs/2301.03398)]  
- [2023] Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2310.07937)]  
- [2023] Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach, arXiv [[Paper](https://arxiv.org/abs/2311.13884)]  
- [2023] LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2310.03903)]  
- [2023] ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation, ICML [[Paper](https://proceedings.mlr.press/v202/zhou23r.html)]  
- [2023] Code as Policies: Language Model Programs for Embodied Control, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10160591)]  
- [2022] Multi-Agent Embodied Visual Semantic Navigation With Scene Prior Knowledge, IEEE RAL [[Paper](https://ieeexplore.ieee.org/abstract/document/9691871)]  
- [2022] Multi-Robot Active Mapping via Neural Bipartite Graph Matching, CVPR [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Ye_Multi-Robot_Active_Mapping_via_Neural_Bipartite_Graph_Matching_CVPR_2022_paper.html)]
- [2022] Learning Efficient Multi-agent Cooperative Visual Exploration, ECCV [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-19842-7_29)]


<a name="Deployment" />

## Deployment

### Mamba
- [2024] Visual Mamba: A Survey and New Outlooks, arXiv [[Paper](https://arxiv.org/abs/2404.18861v3)]  [[Code](https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models)] 


 
### Pruning
- [2025] FASP: Fast and Accurate Structured Pruning of Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2501.09412)]
- [2024] A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations, IEEE TPAMI  [[Paper](https://ieeexplore.ieee.org/abstract/document/10643325/)] [[Code](https://github.com/hrcheng1066/awesome-pruning)]
- [2024] Adaptive Principal Components Allocation with the L2,g-regularized Gaussian Graphical Model for Efficient Fine-Tuning Large Models, arXiv [[Paper](https://arxiv.org/abs/2412.08592)] [[Code](https://github.com/jzheng20/Course_projects.git)]
- [2024] Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization, arXiv [[Paper](https://arxiv.org/abs/2409.18850)]  [[Code](https://github.com/usamec/double_sparse)]
- [2024] Fast and Effective Weight Update for Pruned Large Language Models, TMLR [[Paper](https://openreview.net/forum?id=1hcpXd9Jir)] [[Code](https://github.com/fmfi-compbio/admm-pruning)]
- [2024] A Simple and Effective Pruning Approach for Large Language Models, ICLR [[Paper](https://arxiv.org/abs/2306.11695)] [[Code](https://github.com/locuslab/wanda)]
- [2024] Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models, ICLR [[Paper](https://openreview.net/forum?id=Tr0lPx9woF)] 
- [2024] BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation, arXiv [[Paper](https://arxiv.org/abs/2402.16880)]
- [2024] MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models, NeurIPS [[Paper](https://arxiv.org/abs/2409.17481)] 
- [2024] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs, ICLR [[Paper](https://arxiv.org/abs/2310.08915)] 
- [2023] LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning, arXiv [[Paper](https://doi.org/10.48550/arXiv.2305.18403)]
- [2023] LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation, ICML [[Paper](https://proceedings.mlr.press/v202/li23ap.html)] [[Code](https://github.com/yxli2123/LoSparse)]
- [2023] SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot, ICML [[Paper](https://arxiv.org/abs/2301.00774)] [[Code](https://github.com/IST-DASLab/sparsegpt)]
- [2020] Group Sparsity: The Hinge Between Filter Pruning and Decomposition for Network Compression, CVPR [[Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Group_Sparsity_The_Hinge_Between_Filter_Pruning_and_Decomposition_for_CVPR_2020_paper.html)] [[Code](https://github.com/ofsoundof/group_sparsity)]



### Quantization 
- [2024] A Survey of Low-Bit Large Language Models: Basics, Systems, and Algorithms, arXiv [[Paper](https://arxiv.org/abs/2409.16694)]
- [2024] OneBit: Towards Extremely Low-bit Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2402.11295)] 
- [2024] I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2405.17849)] 
- [2024] Evaluating Quantized Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2402.18158)]
- [2024] The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits, arXiv [[Paper](https://arxiv.org/abs/2402.17764)]
- [2023] Post-training Quantization for Neural Networks with Provable Guarantees, SIMODS [[Paper](https://epubs.siam.org/doi/abs/10.1137/22M1511709)]
- [2023] Training and inference of large language models using 8-bit floating point, arXiv [[Paper](https://arxiv.org/abs/2309.17224)]
- [2023] BitNet: Scaling 1-bit Transformers for Large Language Models, arXiv[[Paper](https://arxiv.org/abs/2310.11453)]
- [2023] LLM-QAT: Data-Free Quantization Aware Training for Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2305.17888)] [[Code](https://github.com/facebookresearch/LLM-QAThttps://github.com/facebookresearch/LLM-QAT)]
- [2023] GPTQ: Accurate Quantization for Generative Pre-trained Transformers, ICLR [[Paper](https://openreview.net/forum?id=tcbBPnfwxS)] [[Code](https://github.com/IST-DASLab/gptq)]
- [2023] QuIP: 2-Bit Quantization of Large Language Models With Guarantees, arXiv [[Paper](https://arxiv.org/abs/2307.13304)] [[Code](https://github.com/jerry-chee/QuIP)]
- [2023] AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration,  arXiv  [[Paper](https://arxiv.org/abs/2306.00978)] [[Code](https://github.com/mit-han-lab/llm-awq)]
- [2023] OWQ: Lessons Learned from Activation Outliers for Weight Quantization in Large Language Models, arXiv </ins> [[Paper](https://arxiv.org/abs/2306.02272)] [[Code](https://github.com/xvyaward/owq)]
- [2023] SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression, arXiv [[Paper](https://arxiv.org/pdf/2306.03078)] [[Code](https://github.com/Vahe1994/SpQR)]
- [2023] QuantEase: Optimization-based Quantization for Language Models, arXiv [[Paper](https://arxiv.org/abs/2309.01885)] [[Code](https://github.com/linkedin/QuantEase)]
- [2023] ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation, arXiv [[Paper](https://arxiv.org/abs/2303.08302)] [[Code](https://github.com/microsoft/DeepSpeed)]
- [2023] QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2310.08041)]
- [2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models, ICML [[Paper](https://arxiv.org/abs/2211.10438)] [[Code](https://github.com/mit-han-lab/smoothquant)]
- [2022] ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers, NeurIPS [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/adf7fa39d65e2983d724ff7da57f00ac-Abstract-Conference.html)]
- [2022] Compression of Generative Pre-trained Language Models via Quantization, ACL [[Paper](https://aclanthology.org/2022.acl-long.331.pdf)]



### Binarization

- [2024] Scalable Binary Neural Network Applications in Oblivious Inference, ACM TECS [[Paper](https://dl.acm.org/doi/full/10.1145/3607192)]
- [2022] Structured Binary Neural Networks for Image Recognition, IJCV [[Paper](https://link.springer.com/article/10.1007/s11263-022-01638-0)]
- [2022] Data-Adaptive Binary Neural Networks for Efficient Object Detection and Recognition, PRL[[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0167865521004438)]
- [2021] Learning Frequency Domain Approximation for Binary Neural Networks, NeurIPS [[Paper](https://proceedings.neurips.cc/paper/2021/hash/d645920e395fedad7bbbed0eca3fe2e0-Abstract.html)]
- [2021] Layer-Wise Searching for 1-Bit Detectors, CVPR [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Layer-Wise_Searching_for_1-Bit_Detectors_CVPR_2021_paper.html)]
- [2020] Rotated Binary Neural Network, NeurIPS [[Paper](https://proceedings.neurips.cc/paper/2020/hash/53c5b2affa12eed84dfec9bfd83550b1-Abstract.html)]
- [2020] Binary Neural Networks: A Survey, PR  [[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320320300856)]
- [2018] Regularized Binary Network Training, arXiv [[Paper](https://arxiv.org/abs/1812.11800)]
- [2018] Enhancing the Performance of 1-bit CNNs with Improved Representational Capability and Advanced Training Algorithm, ECCV [[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/zechun_liu_Bi-Real_Net_Enhancing_ECCV_2018_paper.pdf)]
- [2018] Circulant Binary Convolutional Networks: Enhancing the Performance of 1-bit DCNNs with Circulant Back Propagation, CVPR [[Paper](http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Circulant_Binary_Convolutional_Networks_Enhancing_the_Performance_of_1-Bit_DCNNs_CVPR_2019_paper.html)]
- [2017] How to Train a Compact Binary Neural Network with High Accuracy? AAAI [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/10862)]
- [2015] BinaryConnect: Training Deep Neural Networks with Binary Weights During Propagations, NIPS [[Paper](https://proceedings.neurips.cc/paper_files/paper/2015/file/3e15cc11f979ed25912dff5b0669f2cd-Paper.pdf)]


### Distillation
- [2025] Continual Learning With Knowledge Distillation: A Survey, IEEE TNNLS [[Paper](https://ieeexplore.ieee.org/document/10721446)]








<a name="Tools" />

## Tools
- Hugging Face [[Link](https://huggingface.co/)]  
- LLM-Action [[Link](https://github.com/liguodongiot/llm-action)]  


