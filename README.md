# Large Language Models for Robotics (LLM4RO) 

I currently focus on large language models for robotics (LLM4RO) including
- [Surveys](#Surveys)
- [Perception](#Perception)
- [Planning](#Planning)
- [Control](#Control)



<strong> Last Update: 2025/01/22 </strong>



<a name="Surveys" />

## Surveys
- [2025] Humanoid Locomotion and Manipulation: Current Progress and Challenges in Control, Planning, and Learning, arXiv [[Paper](https://arxiv.org/pdf/2501.02116)] 
- [2025] UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility, arXiv [[Paper](https://arxiv.org/abs/2501.02341)] [[Code](https://github.com/Hub-Tian/UAVs_Meet_LLMs)]
- [2025] A Survey of World Models for Autonomous Driving, arXiv [[Paper](https://arxiv.org/abs/2501.11260)] 
- [2025] Towards Large Reasoning Models: A Survey on Scaling LLM Reasoning Capabilities, arXiv [[Paper](https://arxiv.org/abs/2501.09686)] 
- [2025] A Survey on Large Language Models with some Insights on their Capabilities and Limitations, arXiv [[Paper](https://arxiv.org/abs/2501.04040)]
- [2025] 基于大模型的具身智能系统综述, 自动化学报 [[Paper](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240542)]
- [2025] 具身智能研究的关键问题: 自主感知、行动与进化, 自动化学报 [[Paper](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240364)] [[Code](https://github.com/BUCT-IUSRC/Survey__EmbodiedAI)]
- [2024] 大模型驱动的具身智能: 发展与挑战, 中国科学 [[Paper](https://doi.org/10.1360/SSI-2024-0076)]
- [2024] Personalization of Large Language Models: A Survey, arXiv [[Paper](https://arxiv.org/pdf/2411.00027)] 
- [2024] A Survey on LLM Inference-Time Self-Improvement, arXiv [[Paper](https://arxiv.org/abs/2412.14352)] 
- [2024] Embodied Navigation with Multi-modal Information: A Survey from Tasks to Methodology, Information Fusion [[Paper](https://www.sciencedirect.com/science/article/pii/S1566253524003105)] 
- [2024] Recent Advances in Robot Navigation via Large Language Models: A Review, arXiv [[Paper](https://www.researchgate.net/profile/Xian-Wei-3/publication/384537380)] 
- [2024] Large Language Models for Robotics: Opportunities, Challenges, and Perspectives, arXiv [[Paper](https://arxiv.org/abs/2401.04334)]
- [2024] Advances in Embodied Navigation Using Large Language Models: A Survey, arXiv [[Paper](https://arxiv.org/pdf/2311.00530)]  
- [2024] Foundation Models in Robotics: Applications, Challenges, and the Future, IJRR [[Paper](https://doi.org/10.1177/02783649241281508)] [[Code](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models)]
- [2024] A Survey of Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2303.18223)] [[Code](https://github.com/RUCAIBox/LLMSurvey)]
- [2024] ChatGPT for Robotics: Design Principles and Model Abilities, IEEE Access [[Paper](https://ieeexplore.ieee.org/abstract/document/10500490)] 
- [2023] Large Language Models for Robotics: A Survey, arXiv [[Paper](https://arxiv.org/abs/2311.07226)]




<a name="Perception" />

## Perception

- [2024] OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding, arXiv [[Paper](https://arxiv.org/abs/2404.11000)] 
- [2024] NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models, AAAI [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28597)] 
- [2024] OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments, arXiv [[Paper](https://arxiv.org/abs/2403.09412)]  
- [2023] Chat with the Environment: Interactive Multimodal Perception using Large Language Models, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10342363)]
- [2023] VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models, arXiv [[Paper](https://arxiv.org/abs/2307.05973)]  
- [2023] Steve-Eye: Equipping LLM-Based Embodied Agents with Visual Perception in Open Worlds, ICLR [[Paper](https://arxiv.org/pdf/2310.13255)] 
- [2023] LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action, CORL [[Paper](https://doi.org/10.1177/02783649241281508)] 
- [2022] Flamingo: a Visual Language Model for Few-Shot Learning, NeurIPS [[Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)] 
- [2021] Open-Vocabulary Object Detection via Vision and Language Knowledge Distillation, arXiv [[Paper](https://arxiv.org/pdf/2104.13921)]  



<a name="Planning" />

## Planning

- [2025] LLM-attacker: Enhancing Closed-loop Adversarial Scenario Generation for Autonomous Driving with Large Language Models, arXiv [[Paper](https://arxiv.org/pdf/2501.15850)]   [[Video](https://drive.google.com/file/d/1Zv4V3iG7825oyiKbUwS2Y-rR0DQIE1ZA/view)]
- [2024] The One RING: a Robotic Indoor Navigation Generalist, arXiv [[Paper](https://arxiv.org/pdf/2412.14401)] 
- [2024] Large Language Model guided Deep Reinforcement Learning for Decision Making in Autonomous Driving, arXiv [[Paper](https://arxiv.org/pdf/2412.18511)]
- [2024] LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning, arXiv [[Paper](https://arxiv.org/abs/2407.02511)]  
- [2024] SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments, ICAPS [[Paper](https://ojs.aaai.org/index.php/ICAPS/article/view/31506)] 
- [2024] AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10611163)] 
- [2023] ProgPrompt: Generating Situated Robot Task Plans using Large Language Models, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10161317)] 
- [2023] Text2Motion: from Natural Language Instructions to Feasible Plans, Autonomous Robots [[Paper](https://link.springer.com/article/10.1007/s10514-023-10131-7)] 
- [2023] LLM as A Robotic Brain: Unifying Egocentric Memory and Control, arXiv [[Paper](https://arxiv.org/abs/2304.09349)]  
- [2023] PaLM-E: An Embodied Multimodal Language Model, arXiv [[Paper](https://arxiv.org/abs/2303.03378)]  
- [2022] Do As I Can, Not As I Say: Grounding Language in Robotic Affordances, arXiv [[Paper](https://arxiv.org/abs/2204.01691)]  
- [2022] Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents, ICML [[Paper](https://proceedings.mlr.press/v162/huang22a.html)]  
- [2021] Learning a Decision Module by Imitating Driver’s Control Behaviors, CORL [[Paper](https://proceedings.mlr.press/v155/huang21a.html)]  
- [2021] Neuro-Symbolic Program Search for Autonomous Driving Decision Module Design, CORL [[Paper](https://proceedings.mlr.press/v155/sun21a.html)]  
- [2021] A Lifelong Learning Approach to Mobile Robot Navigation, IEEE RAL [[Paper](https://ieeexplore.ieee.org/abstract/document/9345478)]  



<a name="Control" />

## Control
- [2024] NaVILA: Legged Robot Vision-Language-Action Model for Navigation, arXiv [[Paper](https://arxiv.org/abs/2412.04453)]  [[Video](https://navila-bot.github.io/)] 
- [2024] Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation, arXiv [[Paper](https://arxiv.org/abs/2403.08282)]  
- [2024] GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment, arXiv [[Paper](https://arxiv.org/abs/2403.11075)]  
- [2024] Probabilistically Correct Language-based Multi-Robot Planning using Conformal Prediction, arXiv [[Paper](https://arxiv.org/abs/2402.15368)]  
- [2024] Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration, arXiv [[Paper](https://arxiv.org/abs/2406.14097)]
- [2024] Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610676)]  
- [2024] LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination, AAMAS [[Paper](https://arxiv.org/abs/2312.15224)]  
- [2024] VELMA: Verbalization Embodiment of LLM Agents for Vision and Language Navigation in Street View, AAAI [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/29858)]  
- [2024] SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning, arXiv [[Paper](https://arxiv.org/abs/2403.15648)]
- [2024] RoCo: Dialectic Multi-Robot Collaboration with Large Language Models, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610855)]  
- [2024] Building Cooperative Embodied Agents Modularly with Large Language Models, ICLR [[Paper](https://arxiv.org/abs/2307.02485)]  
- [2024] Lifelong Robot Learning with Human Assisted Language Planners, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10610225)]  
- [2024] MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning, arXiv [[Paper](https://arxiv.org/abs/2402.11260)]
- [2024] LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10802075)]
- [2023] Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model, arXiv [[Paper](https://arxiv.org/abs/2305.11176)]  
- [2023] NaviSTAR: Socially Aware Robot Navigation with Hybrid Spatio-Temporal Graph Transformer and Preference Learning, IROS [[Paper](https://ieeexplore.ieee.org/abstract/document/10341395)]
- [2023] Asynchronous Multi-Agent Reinforcement Learning for Efficient Real-Time Multi-Robot Cooperative Exploration, arXiv [[Paper](https://arxiv.org/abs/2301.03398)]  
- [2023] Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2310.07937)]  
- [2023] Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach, arXiv [[Paper](https://arxiv.org/abs/2311.13884)]  
- [2023] LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2310.03903)]  
- [2023] ESC: Exploration with Soft Commonsense Constraints for Zero-shot Object Navigation, ICML [[Paper](https://proceedings.mlr.press/v202/zhou23r.html)]  
- [2023] Code as Policies: Language Model Programs for Embodied Control, ICRA [[Paper](https://ieeexplore.ieee.org/abstract/document/10160591)]  
- [2022] Multi-Agent Embodied Visual Semantic Navigation With Scene Prior Knowledge, IEEE RAL [[Paper](https://ieeexplore.ieee.org/abstract/document/9691871)]  
- [2022] Multi-Robot Active Mapping via Neural Bipartite Graph Matching, CVPR [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Ye_Multi-Robot_Active_Mapping_via_Neural_Bipartite_Graph_Matching_CVPR_2022_paper.html)]
- [2022] Learning Efficient Multi-agent Cooperative Visual Exploration, ECCV [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-19842-7_29)]


