# Large Language Models for Robotics (LLM4RO)

I currently focus on compression and optimization for large language models (OPT4LLM) including
- [Surveys](#Surveys)
- [Perception](#Perception)
- [Planning](#Planning)
- [Control](#Control)



<strong> Last Update: 2025/01/21 </strong>



<a name="Surveys" />

## Surveys

- [2024] Recent Advances in Robot Navigation via Large Language Models: A Review, arXiv [[Paper](https://www.researchgate.net/profile/Xian-Wei-3/publication/384537380)] 
- [2024] Large Language Models for Robotics: Opportunities, Challenges, and Perspectives, arXiv [[Paper](https://arxiv.org/abs/2401.04334)]
- [2024] Advances in Embodied Navigation Using Large Language Models: A Survey, arXiv [[Paper](https://arxiv.org/pdf/2311.00530)]  
- [2024] Foundation Models in Robotics: Applications, Challenges, and the Future, IJRR [[Paper](https://doi.org/10.1177/02783649241281508)] [[Code](https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models)]
- [2024] A Survey of Large Language Models, arXiv [[Paper](https://arxiv.org/abs/2303.18223)] [[Code](https://github.com/RUCAIBox/LLMSurvey)]
- [2023] Large Language Models for Robotics: A Survey, arXiv [[Paper](https://arxiv.org/abs/2311.07226)]  

Sergey Levine and Dhruv Shah. “Learning
robotic navigation from experience: principles,
methods and recent results”. In: Philosophical
Transactions of the Royal Society B 378.1869
(2023), p. 20210447.
[2] HS Hewawasam, M Yousef Ibrahim, and
Gayan Kahandawa Appuhamillage. “Past,
present and future of path-planning algorithms for mobile robot navigation in dynamic environments”. In: IEEE Open Journal
of the Industr

[15] Jinzhou Lin, Han Gao, Rongtao Xu, et al.
“The development of llms for embodied navigation”. In: arXiv preprint arXiv:2311.00530
(2023).
[16] Jiaqi Wang, Zihao Wu, Yiwei Li, et al.
“Large language models for robotics: Opportunities, challenges, and perspectives”. In:
arXiv preprint arXiv:2401.04334 (2024).

[31] Duy Nguyen-Tuong and Jan Peters. “Model
learning for robot control: a survey”. In: Cognitive processing 12 (2011), pp. 319–340.
[32] Francisco Rubio, Francisco Valero, and Carlos Llopis-Albert. “A review of mobile robots:
Concepts, methods, theoretical framework,
and applications”. In: International Journal
of Advanced Robotic Systems 16.2 (2019),
p. 1729881419839596.
[33] Frank L Lewis and Shuzhi Sam Ge. Autonomous mobile robots: sensing, control, decision ma


43] Kevin Osanlou, Christophe Guettier, Tristan
Cazenave, and Eric Jacopin. “Planning and
learning: A review of methods involving pathplanning for autonomous vehicles”. In: arXiv
preprint arXiv:2207.13181 (2022).
[44] Anis Naema Atiyah Rafai, Noraziah Adzhar,
and Nor Izzati Jaini. “A review on path planning and obstacle avoidance algorithms for
autonomous mobile robots”. In: Journal of
Robotics 2022.1 (2022), p. 2538220.

Yuchen Wu, Pengcheng Zhang, Meiying Gu,
et al. “Embodied navigation with multimodal information: A survey from tasks to
methodolog

<a name="Perception" />

## Perception

[54] Dhruv Shah, B la˙zej Osi´nski, Sergey Levine,
et al. “Lm-nav: Robotic navigation with large
pre-trained models of language, vision, and
action”. In: Conference on robot learning.
PMLR. 2023, pp. 492–504.
[55] Jean-Baptiste Alayrac, Jeff Donahue, Pauline
Luc, et al. “Flamingo: a visual language model
for few-shot learning”. In: Advances in neural information processing systems 35 (2022),
pp. 23716–23736.

[56] Yinan Deng, Jiahui Wang, Jingyu Zhao, et
al. “OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale
Outdoor Environments”. In: arXiv preprint
arXiv:2403.09412 (2024).
[57] Xufeng Zhao, Mengdi Li, Cornelius Weber,
et al. “Chat with the environment: Interactive multimodal perception using large language models”. In: 2023 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). IEEE. 2023, pp. 3590–3596.
[58] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and
Yin Cui. “Open-vocabulary object detection
via vision and language knowledge distillation”. In: arXiv preprint arXiv:2104.13921
(2021).
[59] Sipeng Zheng, Yicheng Feng, Zongqing Lu,
et al. “Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open
Worlds”. In: The Twelfth International Conference on Learning Representations. 2023.
[60] Gengze Zhou, Yicong Hong, and Qi Wu.
“Navgpt: Explicit reasoning in vision-andlanguage navigation with large language models”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. 2024,
pp. 7641–7649.
[61] Edmond Tong, Anthony Opipari, Stanley Lewis, et al. “OVAL-Prompt: OpenVocabulary Affordance Localization
for Robot Manipulation through LLM
Affordance-Grounding”. In: arXiv preprint
arXiv:2404.11000 (2024).
[62] Paola Ard´on, Eric Pairet, Katrin S Lohan, et `
al. “Affordances in robotic tasks–a survey”.
In: arXiv preprint arXiv:2004.07400 (2020).
[25] Wenlong Huang, Chen Wang, Ruohan Zhang,
et al. “Voxposer: Composable 3d value maps
for robotic manipulation with language models”. In: arXiv preprint arXiv:2307.05973
(2023).

<a name="Planning" />

## Planning
[22] Michael Ahn, Anthony Brohan, Noah Brown,
et al. “Do as i can, not as i say: Grounding language in robotic affordances”. In: arXiv
preprint arXiv:2204.01691 (2022).
[23] Danny Driess, Fei Xia, Mehdi SM Sajjadi, et al. “Palm-e: An embodied multimodal language model”. In: arXiv preprint
arXiv:2303.03378 (2023).
[24] Wenlong Huang, Pieter Abbeel, Deepak
Pathak, and Igor Mordatch. “Language models as zero-shot planners: Extracting actionable knowledge for embodied agents”. In: International conference on machine learning.
PMLR. 2022, pp. 9118–9147
[63] Junning Huang, Sirui Xie, Jiankai Sun, et
al. “Learning a decision module by imitating
driver’s control behaviors”. In: Conference on
Robot Learning. PMLR. 2021, pp. 1–10.
[64] Jiankai Sun, Hao Sun, Tian Han, and Bolei
Zhou. “Neuro-symbolic program search for autonomous driving decision module design”. In:
Conference on Robot Learning. PMLR. 2021,
pp. 21–30.
[65] Yongchao Chen, Jacob Arkin, Charles Dawson, et al. “Autotamp: Autoregressive task
and motion planning with llms as translators and checkers”. In: 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2024, pp. 6695–6702.
[66] Roya Firoozi, Johnathan Tucker, Stephen
Tian, et al. “Foundation models in robotics:
Applications, challenges, and the future”. In:
arXiv preprint arXiv:2312.07843 (2023).
[67] Ishika Singh, Valts Blukis, Arsalan Mousavian, et al. “Progprompt: Generating situated
robot task plans using large language models”. In: 2023 IEEE International Conference
on Robotics and Automation (ICRA). IEEE.
2023, pp. 11523–11530.
[68] Thomas Dean. “High-Level Planning And
Low-Level Control”. In: Intelligent Robots and
Computer Vision VI. Vol. 848. SPIE. 1988,
pp. 496–501.
[69] Kevin Lin, Christopher Agia, Toki Migimatsu, et al. “Text2motion: From natural language instructions to feasible plans”. In: Autonomous Robots 47.8 (2023), pp. 1345–1365.
[70] Jinjie Mai, Jun Chen, Bing-chuan Li, et al.
“LLM as A Robotic Brain: Unifying Egocentric Memory and Control”. In: ArXiv
abs/2304.09349 (2023). url: https : / /
api . semanticscholar . org / CorpusID :
258212642.
[71] Abhinav Rajvanshi, Karan Sikka, Xiao Lin,
et al. “SayNav: Grounding Large Language
Models for Dynamic Planning to Navigation
in New Environments”. In: Proceedings of the
International Conference on Automated Planning and Scheduling 34.1 (2024), pp. 464–474.
doi: 10.1609/icaps.v34i1.31506.
[72] Mateus Mendes, A Paulo Coimbra, and
Manuel M Crisostomo. “Robot navigation
based on view sequences stored in a sparse distributed memory”. In: Robotica 30.4 (2012),
pp. 571–581.
[73] Bo Liu, Xuesu Xiao, and Peter Stone. “A lifelong learning approach to mobile robot navigation”. In: IEEE Robotics and Automation
Letters 6.2 (2021), pp. 1090–1096.
[74] Sascha Jockel, Mateus Mendes, Jianwei
Zhang, et al. “Robot navigation and manipulation based on a predictive associative memory”. In: 2009 IEEE 8th International Conference on Development and Learning. IEEE.
2009, pp. 1–7.
[75] Jinsheng Yuan, Wei Guo, Fusheng Zha, et
al. “A bionic spatial cognition model and
method for robots based on the hippocampus
mechanism”. In: Frontiers in Neurorobotics 15
(2022), p. 769829.



<a name="Control" />

## Control
76] Siyuan Huang, Zhengkai Jiang, Hao Dong,
et al. “Instruct2act: Mapping multi-modality
instructions to robotic actions with large
language model”. In: arXiv preprint
arXiv:2305.11176 (2023).
[77] Sai H Vemprala, Rogerio Bonatti, Arthur
Bucker, and Ashish Kapoor. “Chatgpt for
robotics: Design principles and model abilities”. In: IEEE Access (2024).
[78] Jerry W Mao. “A Framework for LLM-based
Lifelong Learning in Robot Manipulation”.
PhD thesis. Massachusetts Institute of Technology, 2024.
[79] Meenal Parakh, Alisha Fong, Anthony Simeonov, et al. “Lifelong robot learning with human assisted language planners”. In: CoRL
2023 Workshop on Learning Effective Abstractions for Planning (LEAP). 2023.
[80] Shu Yang, Muhammad Asif Ali, Cheng-Long
Wang, et al. “MoRAL: MoE Augmented
LoRA for LLMs’ Lifelong Learning”. In:
ArXiv abs/2402.11260 (2024). url: https :
/ / api . semanticscholar . org / CorpusID :
267751418.

81] Chak Lam Shek, Xiyang Wu, Dinesh
Manocha, et al. “LANCAR: Leveraging Language for Context-Aware Robot Locomotion
in Unstructured Environments”. In: arXiv
preprint arXiv:2310.00481 (2023).
[82] Jun Nakanishi, Shunki Itadera, Tadayoshi
Aoyama, and Yasuhisa Hasegawa. “Towards
the development of an intuitive teleoperation
system for human support robot using a VR
device”. In: Advanced Robotics 34.19 (2020),
pp. 1239–1253.
[83] Haokun Liu, Yaonan Zhu, Kenji Kato, et al.
“Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration”.
In: arXiv preprint arXiv:2406.14097 (2024).
[84] Fengpei Yuan, Marie Boltz, Dania Bilal,
et al. “Cognitive exercise for persons with
Alzheimer’s disease and related dementia using a social robot”. In: IEEE Transactions on
Robotics 39.4 (2023), pp. 3332–3346.
[85] Weizheng Wang, Ruiqi Wang, Le Mao, and
Byung-Cheol Min. “Navistar: Socially aware
robot navigation with hybrid spatio-temporal
graph transformer and preference learning”.
In: 2023 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS).
IEEE. 2023, pp. 11348–11355.
[86] Christoforos I Mavrogiannis and Ross A
Knepper. “Multi-agent path topology in support of socially competent navigation planning”. In: The International Journal of
Robotics Research 38.2-3 (2019), pp. 338–356.
[87] Christoforos Mavrogiannis and Ross A Knepper. “Hamiltonian coordination primitives for
decentralized multiagent navigation”. In: The
International Journal of Robotics Research
40.10-11 (2021), pp. 1234–1254.
[88] Pete Trautman, Jeremy Ma, Richard M Murray, and Andreas Krause. “Robot navigation
in dense human crowds: Statistical models
and experimental studies of human–robot cooperation”. In: The International Journal of
Robotics Research 34.3 (2015), pp. 335–356

[89] Gil Manor, Joseph Z Ben-Asher, and Elon Rimon. “Time optimal trajectories for a mobile
robot under explicit acceleration constraints”.
In: IEEE Transactions on Aerospace and Electronic Systems 54.5 (2018), pp. 2220–2232.
[90] Kai Ye, Siyan Dong, Qingnan Fan, et al.
“Multi-robot active mapping via neural bipartite graph matching”. In: Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition. 2022, pp. 14839–
14848.
[91] Chao Yu, Xinyi Yang, Jiaxuan Gao, et al.
“Learning efficient multi-agent cooperative visual exploration”. In: European Conference on
Computer Vision. Springer. 2022, pp. 497–
515.
[92] Xinzhu Liu, Di Guo, Huaping Liu, and Fuchun
Sun. “Multi-agent embodied visual semantic
navigation with scene prior knowledge”. In:
IEEE Robotics and Automation Letters 7.2
(2022), pp. 3154–3161.
[93] Chao Yu, Xinyi Yang, Jiaxuan Gao, et
al. “Asynchronous multi-agent reinforcement
learning for efficient real-time multi-robot
cooperative exploration”. In: arXiv preprint
arXiv:2301.03398 (2023).
[94] Bangguo Yu, Hamidreza Kasaei, and Ming
Cao. “Co-NavGPT: Multi-robot cooperative visual semantic navigation using
large language models”. In: arXiv preprint
arXiv:2310.07937 (2023).
[95] Zhao Mandi, Shreeya Jain, and Shuran Song.
“RoCo: Dialectic Multi-Robot Collaboration
with Large Language Models”. In: 2024 IEEE
International Conference on Robotics and Automation (ICRA). 2024, pp. 286–299. doi: 10.
1109/ICRA57147.2024.10610855.
[96] Hongxin Zhang, Weihua Du, Jiaming Shan,
et al. “Building cooperative embodied agents
modularly with large language models”. In:
arXiv preprint arXiv:2307.02485 (2023).

[97] Bin Zhang, Hangyu Mao, Jingqing Ruan,
et al. “Controlling large language modelbased agents for large-scale decision-making:
An actor-critic approach”. In: arXiv preprint
arXiv:2311.13884 (2023).
[98] Zhonghan Zhao, Kewei Chen, Dongxu Guo,
et al. “Hierarchical auto-organizing system for
open-ended multi-agent navigation”. In: arXiv
preprint arXiv:2403.08282 (2024).
[99] Saaket Agashe, Yue Fan, and Xin Eric Wang.
“Evaluating multi-agent coordination abilities
in large language models”. In: arXiv preprint
arXiv:2310.03903 (2023).
[100] Yongchao Chen, Jacob Arkin, Yang Zhang, et
al. “Scalable Multi-Robot Collaboration with
Large Language Models: Centralized or Decentralized Systems?” In: 2024 IEEE International Conference on Robotics and Automation (ICRA). 2024, pp. 4311–4317. doi: 10.
1109/ICRA57147.2024.10610676.
[101] Jijia Liu, Chao Yu, Jiaxuan Gao, et al.
“Llm-powered hierarchical language agent for
real-time human-ai coordination”. In: arXiv
preprint arXiv:2312.15224 (2023).
[102] Lance Ying, Kunal Jha, Shivam Aarya,
et al. “GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented
Mental Alignment”. In: arXiv preprint
arXiv:2403.11075 (2024).
[103] Jun Wang, Guocheng He, and Yiannis
Kantaros. “Safe Task Planning for LanguageInstructed Multi-Robot Systems using
Conformal Prediction”. In: arXiv preprint
arXiv:2402.15368 (2024).
[104] Kaiwen Zhou, Kaizhi Zheng, Connor Pryor,
et al. “Esc: Exploration with soft commonsense constraints for zero-shot object navigation”. In: International Conference on Machine Learning. PMLR. 2023, pp. 42829–
42842.

[105] Raphael Schumann, Wanrong Zhu, Weixi
Feng, et al. “Velma: Verbalization embodiment of llm agents for vision and language
navigation in street view”. In: Proceedings
of the AAAI Conference on Artificial Intelligence. Vol. 38. 2024, pp. 18924–18933.
[106] Weizheng Wang, Le Mao, Ruiqi Wang,
and Byung-Cheol Min. “SRLM: Human-inLoop Interactive Social Robot Navigation
with Large Language Model and Deep Reinforcement Learning”. In: arXiv preprint
arXiv:2403.15648 (2024).
[107] Shirel Josef and Amir Degani. “Deep reinforcement learning for safe local planning of
a ground vehicle in unknown rough terrain”.
In: IEEE Robotics and Automation Letters 5.4
(2020), pp. 6748–6755.
[108] Xuesu Xiao, Zizhao Wang, Zifan Xu, et al.
“Appl: Adaptive planner parameter learning”.
In: Robotics and Autonomous Systems 154
(2022), p. 104132

26] Jacky Liang, Wenlong Huang, Fei Xia, et al.
“Code as policies: Language model programs
for embodied control”. In: 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE. 2023, pp. 9493–9500.



